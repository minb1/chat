version: '3.8'

services:
  app:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - .:/app
      - huggingface_cache:/root/.cache/huggingface  # Cache HF models

    environment:
      - DEBUG=True
      - DJANGO_SETTINGS_MODULE=chatRAG.settings
      # VLLM configuration - these variables feed into the model_factory.py
      - VLLM_HOST=vllm
      - VLLM_PORT=8080
      - VLLM_MODEL_NAME=mistralai/Mistral-7B-Instruct-v0.3
      # Add PostgreSQL connection details if not already hardcoded
      # - DATABASE_URL=postgres://postgres:postgres@db:5432/logius-standaarden
      # Add Qdrant connection details if needed
      # - QDRANT_URL=http://qdrant:6333
    depends_on:
      - db
      - qdrant
      #- vllm
    # Add labels for Promtail discovery
    labels:
      logging: "promtail"
      application: "django-rag"

  db:
    image: postgres:15
    volumes:
      - postgres_data:/var/lib/postgresql/data/
    environment:
      - POSTGRES_PASSWORD=postgres # Make sure this matches your Django settings if using env vars
      - POSTGRES_USER=postgres
      - POSTGRES_DB=logius-standaarden
    ports:
      - "5432:5432" # Keep exposed for local debugging if needed

  qdrant:
    image: qdrant/qdrant:latest
    volumes:
      - qdrant_data:/qdrant/storage
    ports:
      - "6333:6333"
      - "6334:6334"

  # New VLLM service to run the Llama-3.2-3B-Instruct model
#  vllm:
 #   image: vllm/vllm-openai:latest
  #  restart: unless-stopped
   # environment:
    #  - NVIDIA_VISIBLE_DEVICES=all
#     # - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
 #   volumes:
  #    - huggingface_cache:/root/.cache/huggingface  # Cache HF models
   # ports:
    #  - "8080:8080"  # Expose the OpenAI-compatible API
 #   command: >
  #    --model mistralai/Mistral-7B-Instruct-v0.3
  #    --host 0.0.0.0
   #   --port 8080
    #  --tensor-parallel-size 1
     # --gpu-memory-utilization 0.9
  #    --max-model-len 4096
  #    --dtype bfloat16
  #    --quantization bitsandbytes
#      --served-model-name Llama-3.2-3B-Instruct
  #  deploy:
  #    resources:
  #      reservations:
  #        devices:
  #          - driver: nvidia
  #            count: 1
  #            capabilities: [gpu]

  # --- Monitoring Stack ---
  loki:
    image: grafana/loki:2.9.5 # Use a specific version
    ports:
      - "3100:3100" # Loki API port
    volumes:
      - ./monitoring/config/loki-config.yaml:/etc/loki/local-config.yaml
      - loki_data:/loki # Persist Loki data
    command: -config.file=/etc/loki/local-config.yaml
    deploy:
      resources:
        limits:
          cpus: '2.0'  # Allow up to 2 CPU cores, genuinely wat is dit voor software, absoluut garbage
          memory: 16G   # Allow up to 4GB RAM
        reservations:
          cpus: '1.0'  # Reserve 1 CPU core
          memory: 8G  # Reserve 2GB RAM
    networks:
      - default # Ensure it's on the default network

  promtail:
    image: grafana/promtail:2.9.5 # Use a specific version
    volumes:
      - ./monitoring/config/promtail-config.yaml:/etc/promtail/config.yml
      - /var/lib/docker/containers:/var/lib/docker/containers:ro # Read container logs
      - /var/run/docker.sock:/var/run/docker.sock # Discover containers
      - promtail_positions:/var/log # Persist read positions
    command: -config.file=/etc/promtail/config.yml
    depends_on:
      - loki
      - app # Ensure app starts before promtail tries to find its logs
    networks:
      - default # Ensure it's on the default network

  grafana:
    image: grafana/grafana:10.4.2 # Use a specific version
    ports:
      - "3000:3000" # Grafana UI port
    volumes:
      - grafana_data:/var/lib/grafana
      # Provisioning: Automatically add Loki datasource and dashboard
      - ./monitoring/provisioning/datasources:/etc/grafana/provisioning/datasources
      - ./monitoring/provisioning/dashboards:/etc/grafana/provisioning/dashboards
    environment:
      # Skip login page
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
      # Optional: Set default theme, etc.
      # - GF_USERS_DEFAULT_THEME=dark
    depends_on:
      - loki
    networks:
      - default # Ensure it's on the default network

volumes:
  postgres_data:
  qdrant_data:
  loki_data: # For Loki storage
  promtail_positions: # For Promtail position tracking
  grafana_data: # For Grafana dashboards, plugins, etc.
  huggingface_cache: # Cache for downloaded HF models
